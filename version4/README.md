# Version 4 â€” CPUâ€‘First LangGraph PPT Agent

**Status:** actively working on **CPUâ€‘only model running** and planning a **future user interface**. This version keeps the agentic workflow from earlier versions, but swaps to a **Hugging Face Transformers** stack optimized for CPU execution while we iterate on speed and memory use.

---

## âœ¨ What v4 is
- A LangGraphâ€‘orchestrated **AI agent** that does:
  1) **Read PDFs** (PyMuPDF) with pageâ€‘range support  
  2) **Summarize** (mapâ†’reduce) with numeric detail preservation  
  3) **Plan** a strict **slide outline JSON** (titles, subtitles, 3â€“5 details)  
  4) **Render** a clean PowerPoint via `python-pptx`
- Focused on **running the model on CPU** today; **UI/UX** (desktop/web) comes next.

---

## ğŸ§± Architecture (single file)

```
version4/
â””â”€ ppt_agent.py     # tools + LangGraph state machine + CLI
```

**Key pieces**  
- **Transformers LLM via pipeline:** `AutoModelForCausalLM` + `AutoTokenizer` â†’ `pipeline("text-generation")` â†’ `HuggingFacePipeline` â†’ `ChatHuggingFace`.  
- **Tools:**  
  - `read_pdf(path, pages?, mode?, ...)` â€” fast text extraction with PyMuPDF  
  - `summarize_text(text, style, target_words)` â€” mapâ†’reduce; preserves numbers; fallback logic  
  - `slide_outline_json(notes|topic, slide_target)` â€” JSON schema; validates counts; cleans thin/generic slides; injects quantitative bullets  
  - `build_ppt(outline_json, path)` â€” generates `.pptx` with safe file naming  
- **Agent graph:** `planner_node` (decide next action) â†’ `tool_node` (run one tool & postâ€‘process) â†’ `END`.  
- **CLI:** singleâ€‘turn invoke per prompt; examples printed at startup.

---

## ğŸ–¥ï¸ CPUâ€‘Only: How it runs

This build prefers CPU execution out of the box:

- Default model: `HF_MODEL_ID="Qwen/Qwen2.5-7B-Instruct"` (override with env var).  
- **Quantization OFF on CPU**: set `HF_LOAD_4BIT=0` (default). 4â€‘bit via bitsandbytes is primarily for CUDA; stick to fullâ€‘precision CPU for now.  
- Device mapping is `device_map="auto"`; on a CPUâ€‘only host, it stays on CPU.  
- If your model repo ships an internal **MXFP4 quant config**, the loader detects it and chooses a safe dtype automatically; otherwise we fall back to CPUâ€‘friendly float32.  
- Token budget: `HF_MAX_NEW_TOKENS` controls generation length (defaults to the functionâ€™s `num_predict`).

**Practical tips**  
- Prefer **â‰¤7B** instructionâ€‘tuned models for comfort on typical CPUs.  
- Close other heavy apps; CPU summarization is computeâ€‘bound.  
- If you have plenty of RAM, you can try 13B modelsâ€”expect slower runs.

---

## ğŸ“¦ Requirements

- Python 3.10+ recommended
- `transformers`, `torch`, `langchain-huggingface`, `langgraph`, `langchain-core`
- `pydantic`, `pymupdf` (PyMuPDF), `python-pptx`

Install (example):

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

pip install transformers torch langchain-huggingface langgraph langchain-core pydantic pymupdf python-pptx
```

---

## âš™ï¸ Configuration (env vars)

```bash
# Select the model (CPUâ€‘friendly 7B is a good start)
set HF_MODEL_ID=Qwen/Qwen2.5-7B-Instruct      # Windows (cmd)
export HF_MODEL_ID=Qwen/Qwen2.5-7B-Instruct   # macOS/Linux

# CPU path: keep 4â€‘bit off (bitsandbytes typically requires CUDA)
set HF_LOAD_4BIT=0
export HF_LOAD_4BIT=0

# Control output tokens (optional)
set HF_MAX_NEW_TOKENS=600
export HF_MAX_NEW_TOKENS=600
```

Summary controls (optional):
```bash
# Source clamp (0 = unlimited), chunk size, and max chunks for mapâ†’reduce
export SUMMARY_SOURCE_CAP=0
export SUMMARY_CHUNK_SIZE=3000
export SUMMARY_MAX_CHUNKS=0
```

---

## ğŸš€ Usage

```bash
python ppt_agent.py
```

You can say:
- `summary only for "D:/papers/attention.pdf"`
- `read "D:/papers/paper.pdf" pages 1-3 and summarize`
- `make 10 slides ppt about diffusion models`
- `build slides now`
- `regenerate outline`

**Flow examples**
- **Summaryâ€‘only:** read â†’ summarize â†’ finish (no PPT)  
- **Topic â†’ deck:** outline â†’ build  
- **PDF â†’ deck:** read â†’ summarize â†’ outline â†’ build (or `build slides now` to autoâ€‘chain)

---

## ğŸ§  Outline & Quality Controls

- Enforces exact slide count (default **11**, or parsed from â€œmake 14 slides â€¦â€).  
- **Welcome / Conclusion / Thanks** structure guaranteed.  
- **Numeric facts** are pulled from notes and injected across slides when needed.  
- Cleans generic titles (e.g., â€œTopic 1â€), removes duplicate bullets, and ensures **3â€“5** details per content slide.  
- If a slide is thin, it synthesizes details from notes; on failures, it attempts an LLM densify pass, then deterministic fallback.

---

## ğŸ¯ Roadmap (nearâ€‘term)

- **CPU model running (ongoing):**
  - Tune chunk sizes, reduce memory spikes, and provide timing breakdowns.
  - Optional **sentenceâ€‘window map** for faster summaries on long PDFs.
  - Smarter earlyâ€‘stop during PDF read (chars/pages caps already supported).

- **UI/UX (next):**
  - Minimal desktop UI (PySide/Qt) or a small web UI.
  - File picker, pageâ€‘range helper, theme selection, live progress, and a â€œsummaryâ€‘onlyâ€ export button.

- **PPT beautify:**
  - Themes & templates (`.potx`) and balanced layouts (twoâ€‘column, titleâ€‘only).
  - Consistent font sizing & spacing; optional references slide.

- **Feature growth:**
  - Multiâ€‘document synthesis; citations slide; image/diagram placeholders.
  - Config file (`settings.toml`) for default model/theme/slide count.

---

## ğŸ›  Troubleshooting

- **Slow on CPU** â†’ Use a 7B model; clamp `SUMMARY_CHUNK_SIZE`, consider fewer `SUMMARY_MAX_CHUNKS` during testing.  
- **Out of memory** â†’ Close apps; try a smaller model; shorten PDFs via page ranges (`pages="1-10"`).  
- **Empty or weird text from PDF** â†’ It might be scanned; perform OCR first.  
- **4â€‘bit quant failed** â†’ Keep `HF_LOAD_4BIT=0` on CPUâ€‘only machines.

---

## ğŸ“„ License

MIT (example) â€” adjust to your needs.
