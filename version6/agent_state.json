{
  "topic": "The World of Cats",
  "last_outline": "{\n  \"deck_title\": \"The World of Cats\",\n  \"template\": \"general\",\n  \"slides\": [\n    {\n      \"title\": \"The World of Cats\",\n      \"bullets\": []\n    },\n    {\n      \"title\": \"What Is a Cat?\",\n      \"bullets\": [\n        \"Domestic cat (Felis catus)\",\n        \"Wild relatives: lions, tigers\",\n        \"Family: Felidae\",\n        \"Size: 20-30 cm tall\",\n        \"Lifespan: 12-18 years\"\n      ]\n    },\n    {\n      \"title\": \"Physical Traits\",\n      \"bullets\": [\n        \"Sharp retractable claws\",\n        \"Excellent night vision\",\n        \"Whiskers sense vibrations\",\n        \"Flexible spine\",\n        \"Purring mechanism\"\n      ]\n    },\n    {\n      \"title\": \"Behavioral Traits\",\n      \"bullets\": [\n        \"Territorial marking\",\n        \"Playful hunting instincts\",\n        \"Social grooming\",\n        \"Independent yet affectionate\",\n        \"Nighttime activity\"\n      ]\n    },\n    {\n      \"title\": \"Diet and Nutrition\",\n      \"bullets\": [\n        \"Carnivorous diet\",\n        \"High protein requirement\",\n        \"Essential taurine\",\n        \"Avoid raw eggs\",\n        \"Balanced commercial food\"\n      ]\n    },\n    {\n      \"title\": \"Health and Care\",\n      \"bullets\": [\n        \"Regular vet checkups\",\n        \"Vaccinations: rabies, feline distemper\",\n        \"Dental hygiene\",\n        \"Spaying/neutering\",\n        \"Indoor vs outdoor safety\"\n      ]\n    },\n    {\n      \"title\": \"Common Breeds\",\n      \"bullets\": [\n        \"Siamese: vocal\",\n        \"Persian: long hair\",\n        \"Maine Coon: large\",\n        \"Bengal: spotted\",\n        \"Sphynx: hairless\"\n      ]\n    },\n    {\n      \"title\": \"Cats in Culture\",\n      \"bullets\": [\n        \"Egyptian worship\",\n        \"Japanese folklore\",\n        \"Internet memes\",\n        \"Literary characters\",\n        \"Artistic inspiration\"\n      ]\n    },\n    {\n      \"title\": \"Myth vs Reality\",\n      \"bullets\": [\n        \"Cats are aloof\",\n        \"Cats are independent\",\n        \"Cats can be trained\",\n        \"Cats are good hunters\",\n        \"Cats are affectionate\"\n      ]\n    },\n    {\n      \"title\": \"Adoption Tips\",\n      \"bullets\": [\n        \"Check microchip\",\n        \"Observe behavior\",\n        \"Provide litter box\",\n        \"Gradual introductions\",\n        \"Patience and consistency\"\n      ]\n    },\n    {\n      \"title\": \"Future of Cat Care\",\n      \"bullets\": [\n        \"Genetic research\",\n        \"Smart collars\",\n        \"Telemedicine\",\n        \"Urban cat programs\",\n        \"Sustainable food\"\n      ]\n    },\n    {\n      \"title\": \"Conclusion\",\n      \"bullets\": [\n        \"Cats enrich lives\",\n        \"Responsible ownership\",\n        \"Continued research\",\n        \"Celebrate feline companions\",\n        \"Thank you\"\n      ]\n    }\n  ]\n}",
  "ppt_path": "D:\\work_on_github\\langchain_pptBuilder\\version6\\decks\\The_World_of_Cats_1761378373.pptx",
  "last_text": "Mitigating Paraphrase Attacks on Machine-Text Detectors\nvia Paraphrase Inversion\nRafael Rivera Soto1,2, Barry Chen1, and Nicholas Andrews2\n1Lawrence Livermore National Laboratory\n2Johns Hopkins University\n1rafaelriverasoto@jhu.edu, chen52@llnl.gov, noa@jhu.edu\nAbstract\nHigh-quality paraphrases are easy to produce\nusing instruction-tuned language models or spe-\ncialized paraphrasing models. Although this\ncapability has a variety of benign applications,\nparaphrasing attacks—paraphrases applied to\nmachine-generated texts—are known to signif-\nicantly degrade the performance of machine-\ntext detectors. This motivates us to consider the\nnovel problem of paraphrase inversion, where,\ngiven paraphrased text, the objective is to re-\ncover an approximation of the original text.\nThe closer the approximation is to the original\ntext, the better machine-text detectors will per-\nform. We propose an approach which frames\nthe problem as translation from paraphrased\ntext back to the original text, which requires ex-\namples of texts and corresponding paraphrases\nto train the inversion model. Fortunately, such\ntraining data can easily be generated, given a\ncorpus of original texts and one or more para-\nphrasing models. We find that language mod-\nels such as GPT-4 and Llama-3 exhibit biases\nwhen paraphrasing which an inversion model\ncan learn with a modest amount of data. Per-\nhaps surprisingly, we also find that such models\ngeneralize well, including to paraphrase mod-\nels unseen at training time. Finally, we show\nthat when combined with a paraphrased-text\ndetector, our inversion models provide an effec-\ntive defense against paraphrasing attacks, and\noverall our approach yields an average improve-\nment of +22% AUROC across seven machine-\ntext detectors and three different domains.\n1 Introduction\nRecent developments in the capabilities of large\nlanguage models (LLMs) such as GPT-4 (Ope-\nnAI et al., 2024) have resulted in their widespread\nuse by a variety of users. Although most users\nact responsibly, there is growing concern about\nabuses of LLMs, such as for plagiarism, spam, or\nspreading misinformation (Weidinger et al., 2022;\nHazell, 2023). To minimize the abuse of these\nsystems, several machine-text detection systems\nhave been proposed, including Binoculars (Hans\net al., 2024), FastDetectGPT (Bao et al., 2024),\nand watermarking-based algorithms (Kirchenbauer\net al., 2024; Kuditipudi et al., 2024). However,\nthese systems often fail to detect text that has been\nparaphrased by another model (Krishna et al., 2020;\nSadasivan et al., 2025), leaving a critical gap in cur-\nrent detection systems.\nTo tackle this issue, a recent study has proposed\njointly training a paraphraser and a machine-text\ndetector with an adversarial objective: the para-\nphraser generates text to evade detection, while\nthe detector identifies paraphrased text (Hu et al.,\n2023). Another study has proposed that LLM\nAPI providers cache their generations, enabling\nretrieval over a semantic space, where candidates\nwith high similarity to previous generations are\nmarked as paraphrases (Krishna et al., 2023). Un-\nfortunately, both approaches lack generality, as they\ndepend on training a specialized detector, or having\naccess to all model generations. A more desirable\ndefense would be detector agnostic, improving the\nperformance of any detector.\nIdeally, if the original tokens of a paraphrased\ntext could be recovered, machine-text detectors\nwould perform well, eliminating the need for any\nspecialized solutions. Therefore, we propose the\nnovel task of paraphrase inversion, where the ob-\njective is to recover the original text from a para-\nphrased one. This approach has the added benefit\nof being detector agnostic. Given the space of\npossible paraphrases and the stochastic sampling\nprocedures commonly used, inverting paraphrased\ntext is challenging. Nonetheless, there is evidence\nthat LLMs exhibit consistent biases even when the\ninstruction implicitly or explicitly requests diver-\nsity in the responses (Zhang et al., 2024b; Wu et al.,\n2024).\nEven if paraphrase inversion is possible, we must\nknow when to apply it, making paraphrase detec-\narXiv:2410.21637v3  [cs.CL]  19 Mar 2025\nBLEU(original,inverse)= 57\nOur experiments demonstrate that the proposed technique achieves state-of-the-art results in segmenting brain tumors from MRI scans, demonstrating its effectiveness and potential impact in clinical applications.\nWe show that the proposed method is able toachieve the best results in separating brain tumoursfrom MR images, thereby demonstrating its effectiveness and its practical application.\nWe demonstrate that our proposed technique achieves state-of-the-art results in segmenting brain tumors from MRI scans, highlighting its effectiveness and potential applicability in clinical settings.\nMachine-generated\nParaphrase\nParaphrase  Inversion\nDetected as Machine?  \n✅\nDetected as Machine?  \n❌\nDetected as Machine?  \n✅\nBLEU(original,paraphrase) = 14\nFigure 1: Paraphrasing defeats machine-text detection\nsystem. Our proposed defense (§3) consists of two steps:\n(1) detecting whether text is a paraphrase, and (2) if so,\n(2) inverting the paraphrase back to the original text.\nThis pipeline improves the AUROC of 7 machine-text\ndetectors across three domains by an average of +22%\nAUROC (Table 1).\ntion a necessary step. Detecting text as having\nundergone LLM paraphrasing differs from detect-\ning it as machine-generated, as the original text\nmay have been human-written, in which case large\nportions of the original document may be copies\nof the human-written original. In cases where the\noriginal text is human-written, a machine-text de-\ntector should classify it as such, for example in\ncases where an LLM is used as a writing assistant.\nTo address these concerns, we propose para-\nphrase detection and paraphrase inversion as\na pipeline to improve the performance of any\nmachine-text detector in scenarios where texts may\nhave been paraphrased (Figure 1). Our main con-\ntributions are as follows:\n• We introduce the task of paraphrase inversion\n(§3), where the goal is to recover the origi-\nnal text from a paraphrased one. We formal-\nize the task and provide a comprehensive anal-\nysis of its challenges. We find that inverting\nhuman-written text is significantly harder than\ninverting paraphrases of machine-generated text,\nwhich is to be expected given that human-written\ntext exhibits higher entropy under LLM distribu-\ntions (Gehrmann et al., 2019).\n• We explore two paraphrase detection schemes:\n(1) a simple neural classifier trained to detect\nparaphrased text and (2) an approach that lever-\nages our paraphrase inversion model directly\nwithout requiring an additional model (§3.3).\n• We combine paraphrase detection and paraphrase\ninversion into a single pipeline that improves the\ndetection rate of seven machine-text detectors\nacross three domains (§5.2) by an average of\n+22% AUROC.\nReproducibility The dataset, method implemen-\ntations, model checkpoints, and experimental\nscripts, will be released along with the paper.1\n2 Related Work\nParaphrasing A number of paraphrase corpora\nhave been released over the years which has en-\nabled the development of paraphrase detection and\ngeneration models (Dolan and Brockett, 2005; Gan-\nitkevitch et al., 2013; Wieting and Gimpel, 2018;\nZhang et al., 2019; Krishna et al., 2020). Para-\nphrases have been shown to degrade the perfor-\nmance of machine-text detectors, including those\nbased upon watermarking (Krishna et al., 2023;\nSadasivan et al., 2025). In response to this, sev-\neral defenses have been proposed, including jointly\ntraining a paraphraser and a detector in an adver-\nsarial setting (Hu et al., 2023), building specialized\ndetectors for both the paraphrasing model and the\nlanguage model (Soto et al., 2024), and retrieval\nover a database of semantically similar generations\nproduced by the model in the past (Krishna et al.,\n2023). Paraphrases have also been shown to be\nan effective attack against authorship verification\nsystems (Potthast et al., 2016; Wang et al., 2023),\nallowing bad actors to conceal their identity. To\nour knowledge, our approach is the first attempt\nat inverting the paraphrases, both in general and\nin the context of defending against paraphrasing\nattacks on machine-text detection.\nEmbedding inversion Several lines of work,\nboth in computer vision (Mahendran and Vedaldi,\n2014; Teterwak et al., 2021; Dosovitskiy and Brox,\n2016) and natural language processing (Song and\nRaghunathan, 2020; Li et al., 2023; Morris et al.,\n2023a) have explored whether embeddings can be\ninverted back to their inputs. Prior work has shown\n1Code for all experiments available https://github.\ncom/rrivera1849/inversion\nthat it is possible to recover 92% of 32-token text\ninputs given semantic embeddings (Morris et al.,\n2023b). Moreover, even when the text is isn’t re-\ncovered with high-fidelity, sensitive attributes such\nas the authorship are recoverable (Song and Raghu-\nnathan, 2020). In computer vision, even when an\ninversion model is applied to an adversarially ro-\nbust classifier, enough local and global detail re-\nmains, making the inversion confusable with the\noriginal image, highlighting the difficulty of safe-\nguarding sensitive attributes (Teterwak et al., 2021).\nInverting embeddings is significantly easier than\ninverting paraphrases, as embeddings encode rich\nfeatures of their inputs in continuous latent-space,\nin contrast to the discrete space of paraphrased\ntokens.\nLanguage model inversion (Morris et al.,\n2023b) The objective here is to recover the prompt\nthat generated a particular output. Language model\ninversion techniques such as logit2text (Mor-\nris et al., 2023b) require knowledge of the LLM\nthat generated the output and access to the next-\ntoken probability distribution, making it difficult to\napply in practice. Another approach more closely\nrelated to ours is output2prompt (Zhang et al.,\n2024a), which trains an encoder-decoder architec-\nture to generate the prompt given multiple out-\nputs. However, output2prompt requires up-\nwards of 16 outputs per prompt to successfully\nmatch the performance oflogit2text, and only\nhandles prompts up to 64 tokens long. In contrast\nto these methods, we focus exclusively on inverting\nLLM-generated paraphrases given asingle example\ncleaned of all obvious generation artifacts such as\n“note: I changed... \", thereby removing\nall telltale signs of what the original text might’ve\nbeen. Therefore, the paraphrase inversion problem\nconsidered in this paper is more challenging than\nrelated problems posed in prior work.\n3 Methods\n3.1 Overview\nGiven a text sample yi, we first detect whether it is\na paraphrase using one of our detection schemes.\nIf it is classified as a paraphrase, we apply our\nparaphrase inversion model to recover the original\ntext ˆx ∼ p(. | yi). This sample is then run through\na machine-text detector.\nParaphrase inversion The task of reconstruct-\ning the original source text given paraphrased text.\nThe difficulty of this task hinges in large part on\nassumptions regarding the paraphrasing model. We\nassume access to one or more paraphrasing mod-\nels from which we can generate new paraphrases\n{yi}N\ni=1 given a corpus of N source documents\n{xi}N\ni=1. While access to the paraphrasing models\nin principle affords the possibility of producing an\narbitrary amount of training data, in practice the\nparaphraser may be associated with non-trivial in-\nference costs (e.g., GPT-4). Moreover, even if the\nparaphrasing model is known, the decoding param-\neters such as temperature may not be.2 Therefore, a\nkey question is whether paraphrase inversion mod-\nels generalize to unseen paraphrasers, which we\nconsider in §6.3.\nParaphrase detection The goal is to identify\nwhether a given text is the output of an LLM para-\nphraser, regardless of whether original text was\nhuman-written or machine-generated. Paraphrase\ndetection is crucial for machine-text detection in\nthe wild, where determining when to apply a para-\nphrase inversion model is necessary. We emphasize\nthat detecting text as a paraphrase is not the same\nas identifying text as machine-generated, as the\noriginal text may have been human-written. In\ncases where the original text is human-written, a\nmachine-text detector should classify it as such.\nThis highlights the need of applying a paraphrase\ninversion model to ensure correct detection. How-\never, such a pipeline raises the risk of propagation\nof errors, and we should therefore carefully con-\nsider the cost of such errors.\n1. A false positive occurs when a non-paraphrased\ntext is misidentified as paraphrased. To mini-\nmize the impact of such errors, a robust para-\nphrase inversion model should make minimal\nchanges to the text in such cases. We find that\nour models make significantly fewer changes\nto non-paraphrased documents (§3.3), and that\nthis can in fact be used as a way to distinguish\nbetween paraphrased and non-paraphrased text.\n2. A false negative occurs when a paraphrased\ntext is missed by the detector and we fail to\napply the inversion model. In this case, the\nmachine-text detector is applied to the unmodi-\nfied paraphrased text, which if the original text\nwas machine-generated, is likely to result in\nfalsely predicting that it is human written.\nGiven the above considerations, the paraphrase text\n2We investigate the impact of varying sampling the tem-\nperature during training and inference in Appendix C.\ndetector should aim for high recall at the cost of\npotentially lower precision.\n3.2 End-to-end paraphrase inversion\nTraining objective The inversion models consid-\nered in this paper are fine-tuned using the standard\nsupervised text-to-text objective, fitting an autore-\ngressive conditional language model pθ(yi | xi) on\nthe basis of observed pairs of texts and their para-\nphrases (xi, yi). Our datasets are described in §4.1.\nWe parameterize all our inversion models using\nMistral-7B3, training it with the hyper-parameters\nshown in Appendix F. We use teacher forcing dur-\ning training, conditioning on the the true observed\ntokens.\nInference However good the paraphrasing\nmodel, there may be considerable uncertainty in\nthe distribution over the original text. Therefore we\nsample several inversions and use a scoring func-\ntion to select a single sample which scores highest.\nChoice of score A number of criteria could be\noptimized to help select a single inversion likely\nto be close to the original text. For example, inver-\nsions should retain the meaning of the paraphrased\ntext, and so the score could include a measure of\nsemantic similarity. Furthermore, the inversion\nshould be stylistically distinct from the paraphrased\ntext, as this would indicate a return to the origi-\nnal machine or human styles which are known to\nbe distinct (Soto et al., 2024). In preliminary ex-\nperiments, we found that the paraphrasing model\nconsistently preserved meaning in generated sam-\nples, and so to avoid introducing additional hyper-\nparameters and computational expense, we focus\non stylistic distinctness. Specifically, we compute\na stylistic embedding of the samples and original\ntext to compute a stylistic distance for each can-\ndidate inversion, and select the inversion which is\nfurthest—the most stylistically distinct.\n3.3 Detecting Paraphrases\nNeural paraphrase detector In the simplest\ncase, we train a paraphrase detector dϕ(. | yi)\nusing the standard binary-cross-entropy classifi-\ncation loss. In addition to the standard loss, we\noptimize the model for a paraphrased token predic-\ntion task, where the goal is to determine whether\neach token in a document is copied from the orig-\ninal text or paraphrased. We include this loss to\n3mistralai/Mistral-7B-Instruct-v0.3\n0 100 200 300 400 500 600 700\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nEdit Distance\nHuman T ext\nParaphrases of Machine T ext\nParaphrases of Human T ext\nFigure 2: Edit distances between the original text and its\ninversion when the machine-paraphrase inversion model\nis applied to human-text and paraphrases of human- or\nmachine-text. The inversion model edits human-written\nsignificantly less.\nhelp the model capture the biases that paraphrasers\nintroduce when rewording text. We optimize the\nbinary-cross-entropy for each token, corresponding\nto independent classification decisions. Our model\nis initialized from RoBERTa-large4 (Liu et al.,\n2019), with a multi-layer-perceptron (MLP) head\nthat predicts whether each token was copied from\nthe original text or paraphrased.\nEdit-based paraphrase detector Rather than\ntraining a neural classifier, we determine whether a\nsample yi is a paraphrase based on how many edits\nour paraphrase inversion model makes. Intuitively,\nif the paraphrase inversion model captures LLM\nparaphrasing biases, it should make fewer edits\nwhen “inverting” a human-written text than when\ninverting a paraphrase. Indeed, we find that this is\nthe case in Figure 2. This observation motivates\nthe following paraphrase detection scheme. Given\ntwo Gaussian distributions gh and gm, where gh is\nfit on edit distances of human-text inversions and\ntheir originals and gm on those from paraphrases of\nhuman- and machine-text and their inversions, we\ndetect whether a sample yi is a paraphrase by calcu-\nlating whether yi is more probable under gm than\ngh. This is equivalent to applying a likelihood-ratio\ntest with a threshold of 1. In practice, because we\nhave N inversions per sample, we take the majority\nvote of all such predictions.\n4FacebookAI/roberta-large\n4 Experimental Procedure\n4.1 Datasets\nWe evaluate our approach on three domains:\nReddit, ArXiv, and MovieReviews. We use\nReddit specifically to test the feasibility of para-\nphrase inversion, while all three domains are used\nto evaluate our pipeline for defending machine-text\ndetectors against paraphrase attacks. The valida-\ntion sets of each domain are used to train our edit-\nbased paraphrase detector introduced in §3.3, while\nthe training sets are used to train our paraphrase\ninversion models and our neural paraphrase detec-\ntor. The ArXiv and MovieReviews datasets\nare subsampled from the RAID (Dugan et al.,\n2024) dataset, a machine-text detection bench-\nmark which contains paraphrases of machine-text\nusing DIPPER (Krishna et al., 2023). We re-\nfer to these two datasets as RAID-ArXiv and\nRAID-MovieReviews. The details of how\nRAID was subsampled can be found in Appendix B.\nHere, we discuss how we generate human-text\nparaphrases and machine-text paraphrases for the\nReddit domain, as well as the construction of the\nReddit machine-detection dataset.\nHuman-text paraphrases We use the Reddit\nMillion User Dataset (MUD), which contains com-\nments from over 1 million Reddit users over a\nwide variety of topics (Khan et al., 2021). We\nsubsample the dataset according to the procedure\nin Appendix A. Once subsampled, we gener-\nate the paraphrases of human-text by prompting\nMistral-7B5 (Jiang et al., 2023), Phi-3B 6 (Abdin\net al., 2024), and Llama-3.1-8B 7 (Dubey et al.,\n2024). We clean all obvious LLM-generated\nartifacts such as This rephrased passage\ncondenses, note: I changed... , and\nensure that all paraphrases have a semantic sim-\nilarity of at least 0.7 under SBERT8 (Reimers and\nGurevych, 2019).\nMachine-text paraphrases To generate para-\nphrases of machine-text, we first prompt one of\nthe three LLM at random to produce a response to\neach human-written comment, then we follow the\nsame paraphrasing procedure described above.\nMachine-text detection We combine the test set\nof both our human-text paraphrase and machine-\n5mistralai/Mistral-7B-Instruct-v0.3\n6microsoft/Phi-3-mini-4k-instruct\n7meta-llama/Meta-Llama-3-8B-Instruct\n8sentence-transformers/all-mpnet-base-v2\ntext paraphrase datasets to create a new set com-\nposed of 500 samples in each category: human\ntext, paraphrases of human text, and paraphrases\nof machine text.\n4.2 Metrics\nTo measure how well the inverted text recovers\nthe true tokens, we make use of BLEU (Papineni\net al., 2002), a measure of n-gram overlap. Recov-\nering the original tokens may be difficult, if not\nimpossible. As such, we posit that the inverted\ntext should be close both in style and semantics\nto the original. We measure the stylistic similarity\nby embedding the inversion and the original using\nLUAR (Rivera-Soto et al., 2021)9, a model that cap-\ntures the stylistic features of text; we report the\nstylistic similarity as the cosine similarity between\nthe embeddings. For semantic similarity, we use\nSBERT (Reimers and Gurevych, 2019) to embed\nthe texts and report the cosine similarity between\nthem. To test the performance of the machine-text\ndetectors, we report the area under the curve (AUC)\nof the receiver operating curve (ROC), here denoted\nas AUROC.\n4.3 Baselines\nFor comparison, we prompt GPT-4 to invert the\nparaphrases. We report the prompts used in §D.2.\nAdditionally, we compare our inversion model to\noutput2prompt (Zhang et al., 2024a), train-\ning it on the same dataset. For machine-text de-\ntection, we avail of many popular detectors. We\nuse Rank (Gehrmann et al., 2019), LogRank (So-\nlaiman et al., 2019), Entropy (Ippolito et al.,\n2020), OpenAI’s detector (Solaiman et al., 2019),\nRADAR (Hu et al., 2023), FastDetectGPT (Bao\net al., 2024), and Binoculars (Hans et al., 2024).\n5 Main Results\nThis section present results for our motivating ap-\nplication of defending against paraphrasing attacks\nfor machine-text detection. Next, in §6, we perform\nfurther analysis of individual components of our\napproach, including the feasibility of paraphrase\ninversion as a stand-alone task, considering both in-\nversions of paraphrased machine-generated (§6.1)\nand inversions paraphrased human-written docu-\nments (§6.2).\n9rrivera1849/LUAR-CRUD\nDetector AUROC\nBaseline Inversion+Edit-based Inversion+Neural\nReddit\nOpenAI (2019) 0.56 0.77 0.79\nRank (2019) 0.56 0.66 0.68\nLogRank (2019) 0.58 0.74 0.77\nEntropy (2020) 0.51 0.59 0.59\nRADAR (2023) 0.62 0.66 0.70\nFastDetectGPT (2024) 0.66 0.80 0.84\nBinoculars (2024) 0.77 0.84 0.89\nRAID-ArXiv\nOpenAI (2019) 0.81 0.79 0.77\nRank (2019) 0.71 0.69 0.79\nLogRank (2019) 0.75 0.72 0.91\nEntropy (2020) 0.39 0.42 0.62\nRADAR (2023) 0.99 0.98 0.99\nFastDetectGPT (2024) 0.83 0.78 0.91\nBinoculars (2024) 0.92 0.86 0.98\nRAID-MovieReviews\nOpenAI (2019) 0.82 0.77 0.83\nRank (2019) 0.60 0.76 0.84\nLogRank (2019) 0.66 0.84 0.91\nEntropy (2020) 0.39 0.63 0.71\nRADAR (2023) 0.92 0.92 0.95\nFastDetectGPT (2024) 0.74 0.80 0.89\nBinoculars (2024) 0.91 0.92 0.96\nTable 1: Machine-text detection performance on a dataset of human-text, paraphrases of human-text, and paraphrases\nof machine-text. Applying our inversion model to all samples detected as paraphrases using our paraphrase detection\nschemes (§3.3), we observe significant improvements in detection performance.\nDataset Edit-based Neural\nReddit 0.79 0.94\nRAID-ArXiv 0.52 0.67\nRAID-Reviews 0.79 0.72\nTable 2: F1 scores for the proposed paraphrased detec-\ntion schemes (§3.3).\n5.1 Paraphrase detection\nWe evaluate the proposed paraphrased detection\nschemes described in §3.3. We train the methods\nin all three domains, and report results in Table 2.\nWe find that the neural detector outperforms the\nedit-based detector across two out of three of the\ndomains. Moreover, the edit-based detector per-\nforms poorly in RAID-ArXiv, the most challenging\ndomain, which in turn harms the performance of\nmachine-text detectors in this setting (§5.2).\n5.2 Machine-Text Detection\nWe consider the scenario where human- or\nmachine-text may have been paraphrased by an\nLLM. In this scenario, it would be desirable to label\nparaphrases of human-text as human-written and\nparaphrases of machine-text as machine-generated.\nWe train and evaluate our defense pipeline on\nall three domains separately. We run our para-\nphrase detection schemes on the held-out test set,\ninverting each sample detected as a paraphrase 100\ntimes, and picking the inversion that is the farthest\naway from the input-text in LUAR space, ensur-\ning that the style is dissimilar from paraphrasing\nstyle. We report the AUROC of 7 popular machine-\ntext detectors in Table 1, and make the follow-\ning observations: (1) Our defense, with the neu-\nral paraphrase detector improves the performance\nof 7 machine-text detectors across 3 domains.\nThe only exception is OpenAI’s detector on the\nRAID-ArXiv dataset. (2) RADAR, a detector de-\nsigned to be robust against paraphrase attacks, also\nbenefits. Indeed, in the worst case, RADAR’s per-\nformance remains unchanged ( RADAR-ArXiv),\nbut in other domains, we observe notable improve-\nments. This highlights that our defense can be\ncombined with other existing defenses. (3) The\nedit-based paraphrase detector is not robust across\nall domains. Although the edit-based paraphrase\ndetector improves performance on the Reddit\nand RAID-MovieReviews datasets, it reduces\nperformance on RAID-ArXiv. This decline is\ndue to the many mis-classifications in that domain.\nHowever, overall we observe an average improve-\nMethod Type Machine-written Text Human-written Text\nStyle (↑) Meaning (↑) BLEU (↑) Style (↑) Meaning (↑) BLEU (↑)\nParaphrases - 0.80 0.88 0.17 0.51 0.82 0.08\nBaselines\nGPT-4 Single 0.80 0.85 0.20 0.50 0.80 0.07\nMax 0.86 0.90 0.33 0.56 0.84 0.11\nMean 0.80 0.87 0.21 0.50 0.80 0.07\nout2prompt Single 0.48 0.17 0.00 0.39 0.10 0.00\nMax 0.71 0.40 0.04 0.53 0.32 0.02\nMean 0.48 0.17 0.00 0.39 0.09 0.00\nOurs\nInversion Single 0.84 0.90 0.34 0.54 0.81 0.13\nMax 0.91 0.95 0.51 0.70 0.90 0.25\nMean 0.84 0.90 0.35 0.54 0.81 0.12\nTable 3: Results of inverting paraphrases of machine-written text(left three columns) and paraphrases of human-\nwritten text(right three columns). We generate 100 inversions per sample and report the metrics achieved by a\nsingle inversion, by the best inversion (max), and the average across all inversions. Our proposed inversion model\noutperforms all baselines.\nDetector AUROC\nBaseline Inversion\nTrain - RAID-MovieReviews, Eval - RAID-ArXiv\nOpenAI (2019) 0.81 0.84\nRank (2019) 0.71 0.83\nLogRank (2019) 0.75 0.89\nEntropy (2020) 0.39 0.68\nRADAR (2023) 0.99 0.99\nFastDetectGPT (2024) 0.83 0.90\nBinoculars (2024) 0.92 0.96\nTrain - RAID-ArXiv, Eval - RAID-MovieReviews\nOpenAI (2019) 0.82 0.82\nRank (2019) 0.60 0.83\nLogRank (2019) 0.66 0.90\nEntropy (2020) 0.39 0.68\nRADAR (2023) 0.92 0.94\nFastDetectGPT (2024) 0.74 0.87\nBinoculars (2024) 0.91 0.95\nTable 4: Machine-text detection performance on a\ndataset of human-text, paraphrases of human-text, and\nparaphrases of machine-text. We find that when our\npipeline generalizes even when trained on one do-\nmain, and evaluated on another (e.g. RAID-ArXiv\n→ RAID-MovieReviews).\nment of +22%AUROC averaged across all detectors\nand domains.\n5.3 Generalizing across domains\nDo the paraphrase detection and paraphrase in-\nversion models generalize from one dataset to an-\nother? We apply the pipeline using the neural para-\nphrase detector and inversion model trained on\nRAID-ArXiv to RAID-MovieReviews, and\nvice versa, showing our results in Table 4. We\nfind that our pipeline improves results across all\ndetectors even under these conditions, suggesting\nthat paraphrasers exhibit similar biases regardless\nof what domain they’re applied to.\n6 Further Analysis\n6.1 Inverting paraphrases of\nmachine-generated text\nIn this section, we explore the extent to which para-\nphrases of machine-generated text can be inverted\nto their original tokens. We expect this task to be\neasier than inverting paraphrases of human-written\ntext, as human-written tokens exhibit high entropy\nunder LLM distributions (Gehrmann et al., 2019).\nWe train and evaluate all models on Reddit, gener-\nating 100 inversions per sample on the held-out test\nset and report metrics in Table 3. We observe that\nour model recovers significant portions of the origi-\nnal text, with the best-scoring inversions achieving\nan average BLEU score of 51, with semantic and\nstylistic similarities of 0.95 and 0.91, respectively.\n6.2 Inverting paraphrases of human-written\ntext\nWe now turn to the more difficult problem of in-\nverting paraphrases of human-written text. We\ntrain and evaluate all models on Reddit, generat-\ning 100 inversions per sample on the held-out test\nset and report metrics in Table 3. We highlight\nsome key observations: (1) Inverting paraphrases\nof human-written text is harder than paraphrases\nof machine-generated text, with the best scoring\ninversions achieving an average BLEU score of\n25, which is half of that achieved when inverting\nparaphrases of machine-written text (§6.1). (2)\noutput2prompt does not recover significant\nportions of the original-text, we attribute this to\nMethod Type Style Sim. (↑) Semantic Sim. (↑) BLEU (↑)\nParaphrases - 0.61 0.90 0.21\nInversion Single 0.62 0.88 0.26\nMaximum 0.77 0.94 0.41\nAverage 0.62 0.88 0.26\nTable 5: Inverting GPT-4 paraphrases of human-text, an LLMunseen by the inversion model during training time.\nWe generate 100 inversions per sample, and report the metrics achieved by a single inversion, by the best inversion\n(maximum), and the average across all inversions.\nModel BLEU\nPhi-3 0.08\nMistral-7b 0.11\nLlama-3-8B 0.08\nTable 6: LLMs prompted to invert their own paraphrases\nboth with, and without in-context examples. Generated\n100 inversions per sample, best BLEU score per sample\nshown.\nits requirement of observing multiple outputs per\nprompt, and to the fact that the model has much\nlower capacity than ours (T5-base vs Mistral-7B).\n6.3 Can inversion models invert a novel\nparaphraser?\nTo answer this question, we prompt GPT-4, an un-\nseen LLM during training time, to paraphrase the\nhuman-written Reddit test set. We use our inver-\nsion model trained on Reddit to invert each para-\nphrase 100 times, and report the metrics in Table 5.\nSurprisingly, we find that GPT-4 is easier to in-\nvert than the models seen during training, with\nour model achieving a BLEU score of 41. We at-\ntribute this to GPT-4 paraphrases retaining more of\nthe original text, with its paraphrases achieving a\nBLEU score of 21 in contrast to the BLEU score of\n8 achieved by the LLMs used for training (Table 3).\n6.4 Can an LLM invert its own paraphrases?\nWe prompt each LLM that generated a paraphrase\nin our Reddit dataset to invert its own paraphrase.\nWe generate 100 inversions, and report the average\nmaximum BLEU score achieved in Table 6. Over-\nall, we find when prompted, state-of-the-art LLMs\nare unable to invert their own paraphrase. This\nimplies that even if some parametric knowledge\nencodes the paraphrasing process, the LLM is not\nable to recover the original text given a paraphrase,\nfurther motivating our approach of training para-\nphrase inversion models.\n7 Conclusion\nSummary of findings In this paper, we presented\nthe first detector-agnostic defense against para-\nphrase attacks. This defense relies on the novel\ntask of paraphrase inversion, where the goal is to\nrecover the original tokens of paraphrased text. Fur-\nthermore, we proposed two paraphrase detection\nschemes: one based upon a neural-classifier and\nanother that relies on the number of edits our in-\nversion model makes. When combined with one\nof the proposed paraphrase detectors, our pipeline\nimproves the results of 7 machine-text detectors\nacross 3 domains by an average of +22% AUROC.\nWe attribute the effectiveness of our defense to the\nstylistic similarity of the inverted paraphrases to\nthe original text, which is sufficient for machine\ntext detectors to accurately classify the inverted\ntext. Furthermore, we show that when our defense\nis trained on one domain, it generalizes to another,\nsuggesting that paraphrasers exhibit consistent bi-\nases that can be exploited both for detecting para-\nphrased text and for learning to invert them.\nLimitations\nThe number of paraphrases we use to train our\ninversion models is limited by our compute bud-\nget. We expect that training on additional LLM-\ngenerated paraphrases will improve all the results\nreported in the paper; as such, the results reported\nhere should be viewed as a lower bound on achiev-\nable performance. Our compute budget also pre-\ncluded experimenting with larger local models such\nas Llama-3 70B; however, we do include results\nwith GPT-4 which is of comparable or greater qual-\nity.\nReferences\nMarah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed\nAwadallah, Ammar Ahmad Awan, et al. 2024. Phi-3\ntechnical report: A highly capable language model\nlocally on your phone. Preprint, arXiv:2404.14219.\nGuangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi\nYang, and Yue Zhang. 2024. Fast-detectgpt: Ef-\nficient zero-shot detection of machine-generated\ntext via conditional probability curvature. Preprint,\narXiv:2310.05130.\nBill Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Third international workshop on paraphrasing\n(IWP2005).\nAlexey Dosovitskiy and Thomas Brox. 2016. Inverting\nvisual representations with convolutional networks.\nPreprint, arXiv:1506.02753.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, et al. 2024. The\nllama 3 herd of models. Preprint, arXiv:2407.21783.\nLiam Dugan, Alyssa Hwang, Filip Trhlik, Josh Mag-\nnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ip-\npolito, and Chris Callison-Burch. 2024. Raid:\nA shared benchmark for robust evaluation of\nmachine-generated text detectors. Preprint,\narXiv:2405.07940.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. Ppdb: The paraphrase\ndatabase. In Proceedings of the 2013 conference\nof the north american chapter of the association for\ncomputational linguistics: Human language tech-\nnologies, pages 758–764.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M. Rush. 2019. Gltr: Statistical detection\nand visualization of generated text. Preprint,\narXiv:1906.04043.\nAbhimanyu Hans, Avi Schwarzschild, Valeriia\nCherepanova, Hamid Kazemi, Aniruddha Saha,\nMicah Goldblum, Jonas Geiping, and Tom Goldstein.\n2024. Spotting llms with binoculars: Zero-shot\ndetection of machine-generated text. Preprint,\narXiv:2401.12070.\nJulian Hazell. 2023. Spear phishing with large language\nmodels. Preprint, arXiv:2305.06972.\nXiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. 2023.\nRadar: Robust ai-text detection via adversarial learn-\ning. Preprint, arXiv:2307.03838.\nDaphne Ippolito, Daniel Duckworth, Chris Callison-\nBurch, and Douglas Eck. 2020. Automatic detec-\ntion of generated text is easiest when humans are\nfooled. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1808–1822, Online. Association for Computational\nLinguistics.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023. Mistral 7b. Preprint,\narXiv:2310.06825.\nAleem Khan, Elizabeth Fleming, Noah Schofield, Mar-\ncus Bishop, and Nicholas Andrews. 2021. A\ndeep metric learning approach to account linking.\nPreprint, arXiv:2105.07263.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen,\nJonathan Katz, Ian Miers, and Tom Goldstein. 2024.\nA watermark for large language models. Preprint,\narXiv:2301.10226.\nKalpesh Krishna, Yixiao Song, Marzena Karpinska,\nJohn Wieting, and Mohit Iyyer. 2023. Paraphras-\ning evades detectors of ai-generated text, but retrieval\nis an effective defense. Preprint, arXiv:2303.13408.\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.\nReformulating unsupervised style transfer as para-\nphrase generation. Preprint, arXiv:2010.05700.\nRohith Kuditipudi, John Thickstun, Tatsunori\nHashimoto, and Percy Liang. 2024. Robust\ndistortion-free watermarks for language models.\nPreprint, arXiv:2307.15593.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\nHaoran Li, Mingshi Xu, and Yangqiu Song. 2023.\nSentence embedding leaks more information than\nyou expect: Generative embedding inversion at-\ntack to recover the whole sentence. Preprint,\narXiv:2305.03010.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. Preprint, arXiv:1907.11692.\nAravindh Mahendran and Andrea Vedaldi. 2014. Un-\nderstanding deep image representations by inverting\nthem. Preprint, arXiv:1412.0035.\nJohn X. Morris, V olodymyr Kuleshov, Vitaly Shmatikov,\nand Alexander M. Rush. 2023a. Text embed-\ndings reveal (almost) as much as text. Preprint,\narXiv:2310.06816.\nJohn X. Morris, Wenting Zhao, Justin T. Chiu, Vitaly\nShmatikov, and Alexander M. Rush. 2023b. Lan-\nguage model inversion. Preprint, arXiv:2311.13647.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agar-\nwal, Lama Ahmad, Ilge Akkaya, et al. 2024. Gpt-4\ntechnical report. Preprint, arXiv:2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMartin Potthast, Matthias Hagen, and Benno Stein.\n2016. Author obfuscation: Attacking the state of\nthe art in authorship verification. In Conference and\nLabs of the Evaluation Forum.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nPreprint, arXiv:1908.10084.\nRafael A. Rivera-Soto, Olivia Elizabeth Miano, Juanita\nOrdonez, Barry Y . Chen, Aleem Khan, Marcus\nBishop, and Nicholas Andrews. 2021. Learning uni-\nversal authorship representations. In Proceedings of\nthe 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 913–919, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Bala-\nsubramanian, Wenxiao Wang, and Soheil Feizi. 2025.\nCan ai-generated text be reliably detected? Preprint,\narXiv:2303.11156.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,\nGretchen Krueger, Jong Wook Kim, Sarah Kreps,\nMiles McCain, Alex Newhouse, Jason Blazakis, Kris\nMcGuffie, and Jasmine Wang. 2019. Release strate-\ngies and the social impacts of language models.\nPreprint, arXiv:1908.09203.\nCongzheng Song and Ananth Raghunathan. 2020. In-\nformation leakage in embedding models. Preprint,\narXiv:2004.00053.\nRafael Rivera Soto, Kailin Koch, Aleem Khan, Barry\nChen, Marcus Bishop, and Nicholas Andrews. 2024.\nFew-shot detection of machine-generated text using\nstyle representations. Preprint, arXiv:2401.06712.\nPiotr Teterwak, Chiyuan Zhang, Dilip Krishnan, and\nMichael C. Mozer. 2021. Understanding invariance\nvia feedforward inversion of discriminatively trained\nclassifiers. Preprint, arXiv:2103.07470.\nAndrew Wang, Cristina Aggazzotti, Rebecca Kotula,\nRafael Rivera Soto, Marcus Bishop, and Nicholas\nAndrews. 2023. Can authorship representation\nlearning capture stylistic features? Preprint,\narXiv:2308.11490.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\nCourtney Biles, Sasha Brown, Zac Kenton, Will\nHawkins, Tom Stepleton, Abeba Birhane, Lisa Anne\nHendricks, Laura Rimell, William Isaac, Julia Haas,\nSean Legassick, Geoffrey Irving, and Iason Gabriel.\n2022. Taxonomy of risks posed by language models.\nIn Proceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’22,\npage 214–229, New York, NY , USA. Association for\nComputing Machinery.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-50M:\nPushing the limits of paraphrastic sentence embed-\ndings with millions of machine translations. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 451–462, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nFan Wu, Emily Black, and Varun Chandrasekaran. 2024.\nGenerative monoculture in large language models.\narXiv preprint arXiv:2407.02209.\nCollin Zhang, John X. Morris, and Vitaly Shmatikov.\n2024a. Extracting prompts by inverting llm outputs.\nPreprint, arXiv:2405.15012.\nYiming Zhang, Avi Schwarzschild, Nicholas Carlini,\nZico Kolter, and Daphne Ippolito. 2024b. Forcing\ndiffuse distributions out of language models. arXiv\npreprint arXiv:2404.10859.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPaws: Paraphrase adversaries from word scrambling.\narXiv preprint arXiv:1904.01130.\nA Subsampling the Reddit Dataset\nWe subsample the dataset to au-\nthors who post in r/politics and\nr/PoliticalDiscussion, keeping com-\nments composed of at least 64 tokens but no\nmore than 128 tokens according to the LUAR\ntokenizer. Furthermore, we remove authors with\nless than 10 comments, and randomly sample 10\ncomments from all others, ensuring that no author\nis over-represented.\nTo learn to invert paraphrases, we must observe a\ndiverse set of source documents and corresponding\nparaphrases. However, a random sample of docu-\nments may not provide broad enough coverage of\nwriting styles. For example, when we prompt GPT-\n4 to generate a paraphrase of \"HELLO WORLD\",\nit produces \"Greetings, Universe!\", removing the\ncapital letters. Without observing authors who\nwrite only with capital letters during training, it\nwould be impossible for the inversion model to\ninvert the paraphrase. As such, we split authors\ninto training, validation, and testing splits by sam-\npling authors evenly across the stylistic space. We\nuse LUAR (Rivera-Soto et al., 2021), an embed-\nding that captures stylistic features, to embed each\nauthor’s posts into a single stylistic embedding.\nThen, we cluster the dataset using K-Means, set-\nting K = 100. Finally, we take 80% of the authors\nfrom each cluster for training, 10% for validation,\nand randomly sample 100 authors (2, 449 posts) of\nthose remaining for testing.\nB Creating Datasets from RAID\nIn contrast to our Reddit dataset, the\nRAID (Dugan et al., 2024) benchmark doesn’t\ncontain author-labels. Therefore, sampling\nauthors evenly across stylistic space as in §4.1\nis not possible. RAID contains paraphrases of\nmachine-text using DIPPER, but lacks paraphrases\nof human-text. To address this, we paraphrase all\nhuman-text within ArXiv and MovieReviews\nwith DIPPER, using the same hyper-parameters\nas the creators of RAID (60 lexical diversity,\n0 order diversity, 512 max-tokens). We pair\nup the machine-text with their corresponding\nparaphrases, randomly sampling 80% of these\npairs for training, 10% for validation, and 10% for\ntesting. Furthermore, ensure that the validation\nsets contain an equal number of machine-text and\nparaphrases of machine-text, augmenting them\nwith an equal number of the human-paraphrases\nwe generated. We follow the same procedure for\ntest set, while additionally mensuring that we have\nexactly 500 samples for each category: human-text,\nparaphrases of human-text, and paraphrases of\nmachine-text. The validation sets are used to train\nthe edit-based detector discussed in §3.3, while the\ntraining sets are used to train both our paraphrase\ninversion and paraphrase detection models.\nC Ablations\nHow does varying the sampling procedure im-\npact paraphrase inversion? In Table 7 we show\nthe effect that the decoding temperature has in the\nquality of the inversions generated by our untar-\ngeted inversion model. We generate 100 inversions\nfor every paraphrase in our test dataset, and report\nmetrics using the “max\" scoring strategy dicussed\nin §3. We observe that temperature plays an im-\nportant role in the quality of the inversions, with\nvalues too low or too high significantly degrad-\ning the quality of the inversions. As the tempera-\nture increases, the entropy of the distribution ap-\nproximates that of a uniform distribution, thereby\ndiffusing the style of the inversions. Conversely,\nas the temperature decreases, the inversion model\nbecomes over-confident in its predictive distribu-\ntion, thereby not exploring neighboring tokens and\nstyles.\nTemperature Style Sim. BLEU\n0.3 0.67 0.23\n0.5 0.69 0.24\n0.6 0.70 0.25\n0.7 0.70 0.25\n0.8 0.71 0.24\n0.9 0.71 0.23\n1.5 0.55 0.06\nTable 7: Effect of the temperature in the quality of the\nuntargeted inversions.\nTraining Temperature Style Sim. BLEU\n0.3 0.71 0.26\n0.5 0.70 0.25\n0.7 0.70 0.25\nTable 8: Effect of training on a paraphrase dataset gen-\nerated with different temperature values.\nAre paraphrases generated with lower temper-\nature values easier to invert? To answer this\nquestion, we re-generate our human-text para-\nphrase data with lower temperature values, train-\ning and testing the untargeted inversion model in\nmatched temperature conditions. We report the\nresults in Table 8. We observe that, as the tempera-\nture decreases, the similarity metrics improve. We\nattribute this to the LLMs becoming over-confident\nin their predictive-distribution, thereby generating\nless diverse data which in turn is easier to invert.\nD Prompts\nD.1 Paraphrasing\nWhen paraphrasing with an instruction-tuned LLM,\nwe use the following prompt:\nPrompt:\nRephrase the following passage:\n<PASSAGE>\nOnly output the rephrased-passage,\ndo not include any other details.\nRephrased passage:\nWe also clean out all obvious generation artifacts,\nkeeping only the paraphrased text.\nD.2 Inversion\nD.2.1 Inversion\nPrompt:\n[INST] The following passage\nis a mix of human and machine\ntext, recover the original\nhuman text: {generation}\n[/INST]\\n#####\\nOutput:\n{original}\nD.3 Prompting Inversion\nPrompt:\nThe following passage is a mix of\nhuman and machine text, recover\nthe original human text:\nD.4 Generating Reddit Responses\nPrompt:\nWrite a response to the following\nReddit comment: comment\nE Dataset Statistics\nWe show the statistics of the Reddit,\nRAID-ArXiv, and RAID-MovieReviews\nin Table 9.\nSplit Number of Examples\nReddit Human-Paraphrase\nTrain 204260\nValid 24549\nTest 2449\nReddit Machine-Paraphrase\nTrain 239710\nValid 28883\nTest 2854\nReddit Machine-Text Detection\nTest 1500\nRAID-ArXiv\nTrain 48035\nValid 3798\nTest 1500\nRAID-MovieReviews\nTrain 25649\nValid 1329\nTest 1500\nTable 9: Statistics of the Reddit, RAID-ArXiv, and\nRAID-MovieReviews datasets.\nF Training Hyper-Parameters\nWe train all our inversion models with the hyper-\nparameters shown in Table 10. We train all our\nmodels on 4 NVIDIA-A100 GPUs. Each model\ntook at most 10 hours to train.\nHyper-Parameter Value.\nLearning Rate 2e−5\nNumber of Epochs 4\nLoRA-R 32\nLoRA-α 64\nLoRA-Dropout 0.1\nTable 10: Training Hyper-parameters.\nMost of the compute was spent generating the\ninversions necessary to run all the experiments,\nwhich are in the ballpark of 1M total generations.\nWe used VLLM (Kwon et al., 2023) to speed up\nthe inference time. We estimate an upper bound of\naround 150 GPU hours to run all experiments.",
  "last_summary": "Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase Inversion Rafael Rivera\nSoto1,2, Barry Chen1, and Nicholas Andrews2 1Lawrence Livermore National Laboratory 2Johns Hopkins\nUniversity 1rafaelriverasoto@jhu.edu, chen52@llnl.gov, noa@jhu.edu Abstract High-quality paraphrases\nare easy to produce using instruction-tuned language models or specialized paraphrasing models."
}